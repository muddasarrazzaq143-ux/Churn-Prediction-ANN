{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd68445",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with Neural Network (No sklearn)\n",
    "\n",
    "This notebook trains a neural network **from scratch** on the `Churn_Modelling.csv` dataset, following the style of your intro neural network notebook (manual forward & backprop, no sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a59bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f92d4",
   "metadata": {},
   "source": [
    "## 1. Load & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (make sure Churn_Modelling.csv is in the same folder)\n",
    "df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acf2d7",
   "metadata": {},
   "source": [
    "## 2. Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(\"\\nExited value counts:\")\n",
    "print(df[\"Exited\"].value_counts())\n",
    "print(\"\\nExited value counts (normalized):\")\n",
    "print(df[\"Exited\"].value_counts(normalize=True))\n",
    "\n",
    "df.describe()[[\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40e3c1",
   "metadata": {},
   "source": [
    "## 3. Preprocessing (No sklearn)\n",
    "\n",
    "- Drop ID-like columns\n",
    "- Encode `Gender`\n",
    "- One-hot encode `Geography`\n",
    "- Standardize features\n",
    "- Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop ID columns\n",
    "data = df.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"])\n",
    "\n",
    "# 2. Separate labels\n",
    "y = data[\"Exited\"].to_numpy()\n",
    "X = data.drop(columns=[\"Exited\"]).copy()\n",
    "\n",
    "# 3. Encode Gender as 0/1\n",
    "X[\"Gender\"] = (X[\"Gender\"] == \"Male\").astype(int)\n",
    "\n",
    "# 4. One-hot encode Geography using pandas.get_dummies (still no sklearn)\n",
    "geo_dummies = pd.get_dummies(X[\"Geography\"], prefix=\"Geo\")\n",
    "X = pd.concat([X.drop(columns=[\"Geography\"]), geo_dummies], axis=1)\n",
    "\n",
    "print(\"Features:\", X.columns.tolist())\n",
    "print(\"X shape before scaling:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert to numpy\n",
    "X = X.to_numpy().astype(float)\n",
    "\n",
    "# 6. Standardize features\n",
    "X_mean = X.mean(axis=0, keepdims=True)\n",
    "X_std = X.std(axis=0, keepdims=True)\n",
    "X_std[X_std == 0] = 1.0\n",
    "\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "print(\"Mean (approx):\", X_scaled.mean(axis=0)[:5])\n",
    "print(\"Std (approx):\", X_scaled.std(axis=0)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a22c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train-test split (80/20) using numpy only\n",
    "N = X_scaled.shape[0]\n",
    "rng = np.random.RandomState(0)\n",
    "idx = rng.permutation(N)\n",
    "\n",
    "train_size = int(0.8 * N)\n",
    "train_idx = idx[:train_size]\n",
    "test_idx = idx[train_size:]\n",
    "\n",
    "X_train = X_scaled[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_test = X_scaled[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c4d5c",
   "metadata": {},
   "source": [
    "## 4. Helper Functions (Activations, Loss, Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(H):\n",
    "    return np.maximum(0, H)\n",
    "\n",
    "def sigmoid(H):\n",
    "    return 1 / (1 + np.exp(-H))\n",
    "\n",
    "def softmax(H):\n",
    "    H_shift = H - np.max(H, axis=1, keepdims=True)\n",
    "    expH = np.exp(H_shift)\n",
    "    return expH / np.sum(expH, axis=1, keepdims=True)\n",
    "\n",
    "def one_hot(y, K):\n",
    "    Y = np.zeros((len(y), K))\n",
    "    Y[np.arange(len(y)), y] = 1\n",
    "    return Y\n",
    "\n",
    "def cross_entropy(Y, P_hat):\n",
    "    return -np.mean(np.sum(Y * np.log(P_hat + 1e-9), axis=1))\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return np.mean(y == y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00f717",
   "metadata": {},
   "source": [
    "## 5. General ANN Class (Manual Forward & Backprop)\n",
    "\n",
    "Follows the same style as your neural net notebook: softmax output, cross-entropy loss, ReLU hidden layers by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2352a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, architecture, activations=None):\n",
    "        \"\"\"\n",
    "        architecture: list of hidden layer sizes, e.g. [16, 8]\n",
    "        activations: list of activation functions for each hidden layer\n",
    "                     if None -> all ReLU\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.activations = activations\n",
    "        self.W = {}\n",
    "        self.B = {}\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _init_params(self, D, K):\n",
    "        layers = [D] + self.architecture + [K]\n",
    "        self.L = len(layers) - 1\n",
    "\n",
    "        if self.activations is None:\n",
    "            self.activations = [ReLU] * (self.L - 1)\n",
    "\n",
    "        for l in range(1, self.L + 1):\n",
    "            fan_in = layers[l-1]\n",
    "            fan_out = layers[l]\n",
    "            self.W[l] = 0.1 * np.random.randn(fan_in, fan_out)\n",
    "            self.B[l] = np.zeros(fan_out)\n",
    "\n",
    "    def _forward(self, X):\n",
    "        Z = {0: X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            H = Z[l-1] @ self.W[l] + self.B[l]\n",
    "            if l < self.L:\n",
    "                A = self.activations[l-1](H)\n",
    "            else:\n",
    "                A = softmax(H)\n",
    "            Z[l] = A\n",
    "        return Z\n",
    "\n",
    "    def fit(self, X, y, eta=1e-2, epochs=2000, print_every=200):\n",
    "        np.random.seed(0)\n",
    "        N, D = X.shape\n",
    "        K = len(np.unique(y))\n",
    "\n",
    "        Y = one_hot(y, K)\n",
    "        self._init_params(D, K)\n",
    "        self.loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            Z = self._forward(X)\n",
    "            P_hat = Z[self.L]\n",
    "\n",
    "            loss = cross_entropy(Y, P_hat)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            dH = (P_hat - Y) / N\n",
    "\n",
    "            for l in range(self.L, 0, -1):\n",
    "                dW = Z[l-1].T @ dH\n",
    "                dB = np.sum(dH, axis=0)\n",
    "\n",
    "                self.W[l] -= eta * dW\n",
    "                self.B[l] -= eta * dB\n",
    "\n",
    "                if l > 1:\n",
    "                    dA_prev = dH @ self.W[l].T\n",
    "                    Z_prev = Z[l-1]\n",
    "\n",
    "                    if self.activations[l-2] == ReLU:\n",
    "                        dH = dA_prev * (Z_prev > 0)\n",
    "                    elif self.activations[l-2] == np.tanh:\n",
    "                        dH = dA_prev * (1 - Z_prev**2)\n",
    "                    elif self.activations[l-2] == sigmoid:\n",
    "                        dH = dA_prev * Z_prev * (1 - Z_prev)\n",
    "                    else:\n",
    "                        dH = dA_prev\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Z = self._forward(X)\n",
    "        return Z[self.L]\n",
    "\n",
    "    def predict(self, X):\n",
    "        P_hat = self.predict_proba(X)\n",
    "        return np.argmax(P_hat, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d377c",
   "metadata": {},
   "source": [
    "## 6. Train the ANN on Churn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27060bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "ann = ANN(\n",
    "    architecture=[16, 8],\n",
    "    activations=[ReLU, ReLU]\n",
    ")\n",
    "\n",
    "ann.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eta=1e-2,\n",
    "    epochs=2000,\n",
    "    print_every=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab7f1b",
   "metadata": {},
   "source": [
    "## 7. Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ann.loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss (Cross-Entropy)\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623357b4",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = ann.predict(X_train)\n",
    "y_test_hat  = ann.predict(X_test)\n",
    "\n",
    "train_acc = accuracy(y_train, y_train_hat)\n",
    "test_acc  = accuracy(y_test, y_test_hat)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy:     {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b8259",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix (Test Set, Manual Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    K = len(np.unique(y_true))\n",
    "    cm = np.zeros((K, K), dtype=int)\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        cm[yt, yp] += 1\n",
    "    return cm\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_hat)\n",
    "cm"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
